{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informed-sampling",
   "metadata": {},
   "source": [
    "This notebook tries to reproduce the results from the adversarial spheres paper: https://arxiv.org/pdf/1801.02774.pdf\n",
    "\n",
    "Following guide from https://keras.io/examples/vision/mnist_convnet/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-captain",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "greenhouse-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-computer",
   "metadata": {},
   "source": [
    "### Data generation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "southwest-purple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-7.16985173,  6.97088417],\n",
       "        [ 0.58914738,  0.80802559],\n",
       "        [-0.91550599,  0.40230434],\n",
       "        [ 1.4593279 ,  9.89294507]]),\n",
       " array([1, 0, 0, 1]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_vectors(xs):\n",
    "    return xs / np.linalg.norm(xs, ord=2, axis=1, keepdims=True)\n",
    "\n",
    "def sample_unit_sphere(num_samples, dim):\n",
    "    return normalize_vectors(\n",
    "        np.random.normal(size=(num_samples, dim))\n",
    "    )\n",
    "\n",
    "def generate_sphere_data(dim, rad, num_samples):\n",
    "    assert num_samples % 2 == 0\n",
    "    \n",
    "    xs = np.concatenate([\n",
    "        sample_unit_sphere(num_samples // 2, dim),\n",
    "        rad * sample_unit_sphere(num_samples // 2, dim)\n",
    "    ])\n",
    "    \n",
    "    ys = np.concatenate([\n",
    "        np.zeros(shape=num_samples // 2, dtype=np.int64),\n",
    "        np.ones(shape=num_samples // 2, dtype=np.int64)\n",
    "    ])\n",
    "    \n",
    "    perm = np.random.permutation(num_samples)\n",
    "    xs = xs[perm]\n",
    "    ys = ys[perm]\n",
    "    \n",
    "    return xs, ys\n",
    "\n",
    "generate_sphere_data(dim=2, rad=10, num_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-willow",
   "metadata": {},
   "source": [
    "### Construct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "multiple-house",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 655 ms, sys: 166 ms, total: 822 ms\n",
      "Wall time: 841 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# From https://arxiv.org/pdf/1801.02774.pdf\n",
    "D = 500\n",
    "R = 1.3\n",
    "\n",
    "# CIFAR10 dataset size\n",
    "NUM_TRAIN = 5 * 10 ** 4\n",
    "NUM_TEST = 10 ** 4\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "x_train, y_train = generate_sphere_data(\n",
    "    dim=D,\n",
    "    rad=R,\n",
    "    num_samples=NUM_TRAIN,\n",
    ")\n",
    "\n",
    "x_test, y_test = generate_sphere_data(\n",
    "    dim=D,\n",
    "    rad=R,\n",
    "    num_samples=NUM_TEST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "classical-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 1,504,002\n",
      "Trainable params: 1,504,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=x_train[0].shape),\n",
    "        layers.Dense(1000, activation=\"relu\"),\n",
    "        layers.Dense(1000, activation=\"relu\"),\n",
    "        layers.Dense(2, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smart-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "900/900 [==============================] - 6s 6ms/step - loss: 0.1006 - accuracy: 0.9559 - val_loss: 0.0266 - val_accuracy: 0.9906\n",
      "Epoch 2/5\n",
      "900/900 [==============================] - 6s 6ms/step - loss: 0.0185 - accuracy: 0.9934 - val_loss: 0.0220 - val_accuracy: 0.9934\n",
      "Epoch 3/5\n",
      "900/900 [==============================] - 6s 6ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0281 - val_accuracy: 0.9898\n",
      "Epoch 4/5\n",
      "900/900 [==============================] - 5s 6ms/step - loss: 0.0109 - accuracy: 0.9961 - val_loss: 0.0223 - val_accuracy: 0.9926\n",
      "Epoch 5/5\n",
      "900/900 [==============================] - 5s 6ms/step - loss: 0.0075 - accuracy: 0.9973 - val_loss: 0.0243 - val_accuracy: 0.9906\n",
      "CPU times: user 5min 17s, sys: 51.6 s, total: 6min 8s\n",
      "Wall time: 28.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f559a8502b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"SparseCategoricalCrossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=50,\n",
    "    epochs=5,\n",
    "    validation_split=0.1\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "greenhouse-bunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nat train loss: 0.011119423434138298\n",
      "Nat train accuracy: 0.9960799813270569\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Nat train loss:\", score[0])\n",
    "print(\"Nat train accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "negative-visiting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nat test loss: 0.026690993458032608\n",
      "Nat test accuracy: 0.9905999898910522\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Nat test loss:\", score[0])\n",
    "print(\"Nat test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "level-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/adversarial_retraining.ipynb\n",
    "# and https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/238\n",
    "art_model = TensorFlowV2Classifier(\n",
    "    model=model,\n",
    "    input_shape=x_test[0].shape,\n",
    "    nb_classes=2,\n",
    "    loss_object=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    clip_values=[-R, R],\n",
    ")\n",
    "\n",
    "attack = ProjectedGradientDescent(\n",
    "    art_model,\n",
    "    norm=2,\n",
    "    eps=0.18,\n",
    "    eps_step=0.01,\n",
    "    max_iter=40,\n",
    "    batch_size=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "continuous-liability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv train loss: 10.049074172973633\n",
      "Adv train accuracy: 0.3400000035762787\n",
      "CPU times: user 2min 44s, sys: 13.3 s, total: 2min 57s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "x_train_adv = attack.generate(x_train[:NUM_TEST], y=y_train[:NUM_TEST])\n",
    "\n",
    "score = model.evaluate(x_train_adv, y_train[:NUM_TEST], verbose=0)\n",
    "print(\"Adv train loss:\", score[0])\n",
    "print(\"Adv train accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "finnish-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv test loss: 10.807324409484863\n",
      "Adv test accuracy: 0.2883000075817108\n",
      "CPU times: user 2min 45s, sys: 11.7 s, total: 2min 57s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "x_test_adv = attack.generate(x_test, y=y_test)\n",
    "\n",
    "score = model.evaluate(x_test_adv, y_test, verbose=0)\n",
    "print(\"Adv test loss:\", score[0])\n",
    "print(\"Adv test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-measure",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-lodge",
   "metadata": {},
   "source": [
    "We perform an L2 perturbation of 0.18 on inputs of norm 1.0 or 1.3.\n",
    "\n",
    "However, in real life, images do not have norm 1.0 or 1.3.\n",
    "If we assume pixel intensities in $[0, 1]$ with an average intensity of $0.5$,\n",
    "real life images have a norm of approximately $0.5 \\times \\sqrt{D}$,\n",
    "where $D$ is the number of pixels in the image.\n",
    "\n",
    "Thus the effective L2 perturbation for IRL images is approximately\n",
    "$0.18 \\times 0.5 \\sqrt{D} \\approx 2$.\n",
    "\n",
    "Thus the IRL equivalent of the above experiment\n",
    "is an L2 perturbation of norm 2 resulting in 28.8% adversarial test accuracy\n",
    "(compared to 99% natural test accuracy)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
